from dotenv import load_dotenv
import os
import re
from mistralai import Mistral
from mistralai.models import OCRResponse
from openai import OpenAI
import tiktoken  # OpenAIã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
from tqdm import tqdm  # é€²æ—ãƒãƒ¼ç”¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª


# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’å–å¾—
enc = tiktoken.encoding_for_model("gpt-4o-mini")


def ocr_pdf(api_key, file_name, data_dir):
    if not api_key:
        raise ValueError("MISTRAL_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    # Mistral APIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®ä½œæˆ
    client = Mistral(api_key=api_key)


    pdf_path = f"{data_dir}/{file_name}"

    # PDFã‚’Mistralã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_pdf = client.files.upload(
        file={
            "file_name": file_name,
            "content": open(pdf_path, "rb"),
        },
        purpose="ocr"
    )

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸPDFã®ã‚µã‚¤ãƒ³ä»˜ãURLã‚’å–å¾—
    signed_url = client.files.get_signed_url(file_id=uploaded_pdf.id)

    # OCRå‡¦ç†ã‚’å®Ÿè¡Œ
    ocr_response = client.ocr.process(
        model="mistral-ocr-latest",
        document={
            "type": "document_url",
            "document_url": signed_url.url,
        },
        include_image_base64=True
    )
    return ocr_response

def save_markdown(content, filename="output.md"):
    """ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®å†…å®¹ã‚’æŒ‡å®šã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹"""
    with open(filename, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"Markdown saved to {filename}")


def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:
    for img_name, base64_str in images_dict.items():
        markdown_str = markdown_str.replace(f"![{img_name}]({img_name})", f"![{img_name}]({base64_str})")
    return markdown_str

def get_combined_markdown(ocr_response: OCRResponse) -> str:
  markdowns: list[str] = []
  for page in ocr_response.pages:
    image_data = {}
    for img in page.images:
      image_data[img.id] = img.image_base64
    markdowns.append(replace_images_in_markdown(page.markdown, image_data))
  return "\n\n".join(markdowns)

def split_title_and_reference(content):
    """æœ€åˆã® `#` ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã€`# References` éƒ¨åˆ†ã‚’å‚è€ƒæ–‡çŒ®ã€ãã‚Œä»¥é™ã‚’æœ¬æ–‡ã¨ã—ã¦åˆ†é›¢"""
    lines = content.split("\n")

    title = None
    reference = None
    body_start = 0
    reference_start = None
    next_header_after_reference = None

    # æœ€åˆã® `# ` ã‚’ã‚¿ã‚¤ãƒˆãƒ«ã¨ã™ã‚‹
    for i, line in enumerate(lines):
        if line.startswith("# "):  # æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚¿ã‚¤ãƒˆãƒ«
            title = line.strip()
            body_start = i + 1  # ã‚¿ã‚¤ãƒˆãƒ«ã®æ¬¡ã®è¡Œã‹ã‚‰æœ¬æ–‡é–‹å§‹
            break

    # `# References` ä»¥é™ã‚’å‚è€ƒæ–‡çŒ®ã¨ã—ã¦æ‰±ã†
    for i, line in enumerate(lines):
        if re.match(r"^#{1,2} References", line, re.IGNORECASE):  # `# References` ã¾ãŸã¯ `## References`
            reference_start = i
            break

    # `# References` ã®å¾Œã®æ¬¡ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ¢ã™
    if reference_start is not None:
        for i in range(reference_start + 1, len(lines)):
            if re.match(r"^#{1,6} ", lines[i]):  # `#`, `##`, `###` ãªã©ã®æ¬¡ã®ãƒ˜ãƒƒãƒ€ãƒ¼
                next_header_after_reference = i
                break

    # æœ¬æ–‡ã®å–å¾—
    if reference_start is not None:
        body = "\n".join(lines[body_start:reference_start]).strip()  # `# References` ã‚ˆã‚Šå‰ã®æœ¬æ–‡
        reference = "\n".join(lines[reference_start:next_header_after_reference]).strip() if next_header_after_reference else "\n".join(lines[reference_start:]).strip()
        remaining_body = "\n".join(lines[next_header_after_reference:]).strip() if next_header_after_reference else ""

        # å‚è€ƒæ–‡çŒ®ã‚’ `[1]`, `[2]` ã”ã¨ã«æ”¹è¡Œã™ã‚‹
        if reference:
            reference = re.sub(r"\s*(\[\d+\])", r"\n\1", reference).strip()

    else:
        body = "\n".join(lines[body_start:]).strip()  # å‚è€ƒæ–‡çŒ®ãŒãªã„å ´åˆã¯ã™ã¹ã¦æœ¬æ–‡
        reference = None
        remaining_body = ""

    return title, body, reference, remaining_body



def split_by_headings(content, max_tokens=8000):
    """è¦‹å‡ºã—ã”ã¨ã«ãƒãƒ£ãƒ³ã‚¯ã‚’åˆ†å‰²ã—ã€ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’èª¿æ•´ï¼ˆé€”ä¸­ã§æ–‡ç« ãŒé€”åˆ‡ã‚Œãªã„ã‚ˆã†ã«ã™ã‚‹ï¼‰"""
    sections = re.split(r"(^# .+?$)", content, flags=re.MULTILINE)  # è¦‹å‡ºã—ã§åˆ†å‰²
    chunks = []
    current_chunk = ""
    current_token_count = 0

    def count_tokens(text):
        return len(enc.encode(text))

    for i in range(len(sections)):
        section = sections[i].strip()
        if not section:
            continue

        section_tokens = count_tokens(section)

        # è¿½åŠ ã—ã¦ã‚‚ max_tokens ã‚’è¶…ãˆãªã„å ´åˆã¯ãã®ã¾ã¾è¿½åŠ 
        if current_token_count + section_tokens <= max_tokens:
            current_chunk += "\n" + section if current_chunk else section
            current_token_count += section_tokens
        else:
            # æ—¢ã«ä½•ã‹ãŒã‚ã‚‹ãªã‚‰ã€ã¾ãšä»Šã®ãƒãƒ£ãƒ³ã‚¯ã‚’ç¢ºå®š
            if current_chunk:
                chunks.append(current_chunk.strip())
                current_chunk = ""
                current_token_count = 0

            # ç¾åœ¨ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒ1ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã«åã¾ã‚‹å ´åˆã¯ãã®ã¾ã¾è¿½åŠ 
            if section_tokens <= max_tokens:
                current_chunk = section
                current_token_count = section_tokens
            else:
                # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒé•·ã™ãã‚‹å ´åˆã€æ®µè½å˜ä½ã§ã•ã‚‰ã«åˆ†å‰²
                paragraphs = section.split("\n")
                temp_chunk = ""
                temp_token_count = 0

                for paragraph in paragraphs:
                    paragraph = paragraph.strip()
                    if not paragraph:
                        continue
                    paragraph_tokens = count_tokens(paragraph)

                    # è¿½åŠ ã—ã¦ã‚‚ã‚ªãƒ¼ãƒãƒ¼ã—ãªã„ãªã‚‰ãã®ã¾ã¾è¿½åŠ 
                    if temp_token_count + paragraph_tokens <= max_tokens:
                        temp_chunk += "\n" + paragraph if temp_chunk else paragraph
                        temp_token_count += paragraph_tokens
                    else:
                        # æ—¢ã«ä½•ã‹ãŒã‚ã‚‹ãªã‚‰ã€ç¢ºå®š
                        if temp_chunk:
                            chunks.append(temp_chunk.strip())
                            temp_chunk = ""
                            temp_token_count = 0

                        # ä»Šã®æ®µè½ãŒ1ã¤ã®ãƒãƒ£ãƒ³ã‚¯ã«åã¾ã‚‹ãªã‚‰æ–°è¦ãƒãƒ£ãƒ³ã‚¯ã«ã™ã‚‹
                        if paragraph_tokens <= max_tokens:
                            temp_chunk = paragraph
                            temp_token_count = paragraph_tokens
                        else:
                            # 1æ®µè½ã§ã‚‚ max_tokens ã‚’è¶…ãˆã‚‹ãªã‚‰ã€æ–‡ç« å˜ä½ã§ã•ã‚‰ã«åˆ†å‰²
                            sentences = re.split(r"(?<=[.!?])\s+", paragraph)  # æ–‡å˜ä½ã§åˆ†å‰²
                            sentence_chunk = ""
                            sentence_token_count = 0

                            for sentence in sentences:
                                sentence = sentence.strip()
                                if not sentence:
                                    continue
                                sentence_tokens = count_tokens(sentence)

                                if sentence_token_count + sentence_tokens <= max_tokens:
                                    sentence_chunk += " " + sentence if sentence_chunk else sentence
                                    sentence_token_count += sentence_tokens
                                else:
                                    if sentence_chunk:
                                        chunks.append(sentence_chunk.strip())
                                    sentence_chunk = sentence
                                    sentence_token_count = sentence_tokens

                            if sentence_chunk:
                                chunks.append(sentence_chunk.strip())

                if temp_chunk:
                    chunks.append(temp_chunk.strip())

    if current_chunk:  # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ã‚’è¿½åŠ 
        chunks.append(current_chunk.strip())

    return chunks


# def translate_gpt(ocr_response, api_key_openai):
#     """OCRã®Markdownãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€æ‹¬ã§å‡¦ç†ã—ã€è¦‹å‡ºã—ã”ã¨ã«ãƒãƒ£ãƒ³ã‚¯åŒ–ã—ã¦ç¿»è¨³ã—ã€ç”»åƒã‚’å†é…ç½®ã™ã‚‹"""

#     client = OpenAI(api_key=api_key_openai)

#     print(f"ğŸ“„ {len(ocr_response.pages)}ãƒšãƒ¼ã‚¸ã®ç¿»è¨³ã‚’é–‹å§‹ã—ã¾ã™...")

#     # å…¨ãƒšãƒ¼ã‚¸ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’çµåˆ
#     combined_markdown = get_combined_markdown(ocr_response)

#     # ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æœ¬æ–‡ãƒ»å‚è€ƒæ–‡çŒ®ãƒ»æ®‹ã‚Šã®æœ¬æ–‡ã‚’åˆ†é›¢
#     title, body, reference, remaining_body = split_title_and_reference(combined_markdown)

#     # æœ¬æ–‡ã¨ `# References` ä»¥é™ã®æœ¬æ–‡ã‚’è¦‹å‡ºã—å˜ä½ã§ãƒãƒ£ãƒ³ã‚¯åŒ–
#     body_chunks = split_by_headings(body, max_tokens=8000)
#     remaining_chunks = split_by_headings(remaining_body, max_tokens=8000)
#     chunks = body_chunks + remaining_chunks

#     translated_chunks = []

#     print(f"ğŸ”„ è¦‹å‡ºã—å˜ä½ã®ç¿»è¨³ï¼ˆ{len(chunks)} ãƒãƒ£ãƒ³ã‚¯ï¼‰...")

#     # å„ãƒãƒ£ãƒ³ã‚¯ã‚’GPTã§ç¿»è¨³
#     for chunk_idx, chunk in enumerate(tqdm(chunks, desc="ç¿»è¨³ä¸­", unit="chunk")):
#         try:
#             completion = client.chat.completions.create(
#                 model="gpt-4o-mini",
#                 messages=[
#                     {"role": "system", "content": "å†…å®¹ã®è‹±èªã®æ–‡ç« ã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®å½¢å¼ã‚’å´©ã•ãšã«æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚"},
#                     {"role": "user", "content": chunk},
#                 ],
#             )
#             translated_chunks.append(completion.choices[0].message.content)
#             print(f"âœ… ãƒãƒ£ãƒ³ã‚¯ {chunk_idx + 1} / {len(chunks)} ç¿»è¨³å®Œäº†")

#         except Exception as e:
#             print(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯ {chunk_idx + 1} ã®ç¿»è¨³ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
#             translated_chunks.append(chunk)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨

#     # ç¿»è¨³å¾Œã®æœ¬æ–‡ã‚’å†çµåˆ
#     translated_body = "\n".join(translated_chunks[:len(body_chunks)])
#     translated_remaining_body = "\n".join(translated_chunks[len(body_chunks):])

#     # ã‚¿ã‚¤ãƒˆãƒ«ãƒ»ç¿»è¨³æ¸ˆã¿æœ¬æ–‡ãƒ»å‚è€ƒæ–‡çŒ®ã‚’å†æ§‹æˆ
#     final_markdown = ""
#     if title:
#         final_markdown += title + "\n\n"
#     final_markdown += translated_body + "\n\n"
#     if reference:
#         final_markdown += reference + "\n\n"
#     final_markdown += translated_remaining_body

#     # ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã®ä½ç½®ã«æˆ»ã™
#     image_data = {img.id: img.image_base64 for page in ocr_response.pages for img in page.images}
#     final_markdown = replace_images_in_markdown(final_markdown, image_data)

#     print("\nğŸ‰ ç¿»è¨³å®Œäº†ï¼")
#     return final_markdown


def extract_and_replace_images(markdown):
    """Markdown ã‹ã‚‰ç”»åƒéƒ¨åˆ†ã‚’æŠ½å‡ºã—ã€ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã«ç½®ãæ›ãˆã‚‹"""
    image_pattern = r"(!\[[^\]]*\]\([^\)]+\))"  # `![alt](image_url)`
    images = re.findall(image_pattern, markdown)  # ç”»åƒéƒ¨åˆ†ã‚’æŠ½å‡º
    placeholders = [f"{{IMAGE_{i}}}" for i in range(len(images))] 
    
    # ç”»åƒã‚’ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã«ç½®æ›
    modified_markdown = markdown
    for img, placeholder in zip(images, placeholders):
        modified_markdown = modified_markdown.replace(img, placeholder, 1)

    return modified_markdown, images, placeholders

def restore_images(markdown, images, placeholders):
    """ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’å…ƒã®ç”»åƒã«æˆ»ã™"""
    for img, placeholder in zip(images, placeholders):
        markdown = markdown.replace(placeholder, img, 1)
    return markdown

def translate_gpt(ocr_response, api_key_openai):
    """OCRã®Markdownãƒ†ã‚­ã‚¹ãƒˆã‚’ä¸€æ‹¬ã§å‡¦ç†ã—ã€ç”»åƒã‚’ç¿»è¨³ã›ãšã«ãã®ã¾ã¾ç¶­æŒã™ã‚‹"""

    client = OpenAI(api_key=api_key_openai)

    print(f"ğŸ“„ {len(ocr_response.pages)}ãƒšãƒ¼ã‚¸ã®ç¿»è¨³ã‚’é–‹å§‹ã—ã¾ã™...")

    # å…¨ãƒšãƒ¼ã‚¸ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚’çµåˆ
    combined_markdown = get_combined_markdown(ocr_response)

    # ç”»åƒã‚’ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã«ç½®ãæ›ãˆ
    combined_markdown, images, placeholders = extract_and_replace_images(combined_markdown)

    # ã‚¿ã‚¤ãƒˆãƒ«ãƒ»æœ¬æ–‡ãƒ»å‚è€ƒæ–‡çŒ®ãƒ»æ®‹ã‚Šã®æœ¬æ–‡ã‚’åˆ†é›¢
    title, body, reference, remaining_body = split_title_and_reference(combined_markdown)

    # æœ¬æ–‡ã¨ `# References` ä»¥é™ã®æœ¬æ–‡ã‚’è¦‹å‡ºã—å˜ä½ã§ãƒãƒ£ãƒ³ã‚¯åŒ–
    body_chunks = split_by_headings(body, max_tokens=8000)
    remaining_chunks = split_by_headings(remaining_body, max_tokens=8000)
    chunks = body_chunks + remaining_chunks

    translated_chunks = []

    print(f"ğŸ”„ è¦‹å‡ºã—å˜ä½ã®ç¿»è¨³ï¼ˆ{len(chunks)} ãƒãƒ£ãƒ³ã‚¯ï¼‰...")

    # å„ãƒãƒ£ãƒ³ã‚¯ã‚’GPTã§ç¿»è¨³
    for chunk_idx, chunk in enumerate(tqdm(chunks, desc="ç¿»è¨³ä¸­", unit="chunk")):
        try:
            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {"role": "system", "content": "å†…å®¹ã®è‹±èªã®æ–‡ç« ã‚’ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã®å½¢å¼ã‚’å´©ã•ãšã«æ—¥æœ¬èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚ãŸã ã—ã€{IMAGE_X} ã®ã‚ˆã†ãªãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã¯ãã®ã¾ã¾ç¶­æŒã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": chunk},
                ],
            )
            translated_chunks.append(completion.choices[0].message.content)
            print(f"âœ… ãƒãƒ£ãƒ³ã‚¯ {chunk_idx + 1} / {len(chunks)} ç¿»è¨³å®Œäº†")

        except Exception as e:
            print(f"âš ï¸ ãƒãƒ£ãƒ³ã‚¯ {chunk_idx + 1} ã®ç¿»è¨³ä¸­ã«ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ: {e}")
            translated_chunks.append(chunk)  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½¿ç”¨

    # ç¿»è¨³å¾Œã®æœ¬æ–‡ã‚’å†çµåˆ
    translated_body = "\n".join(translated_chunks[:len(body_chunks)])
    translated_remaining_body = "\n".join(translated_chunks[len(body_chunks):])

    # ã‚¿ã‚¤ãƒˆãƒ«ãƒ»ç¿»è¨³æ¸ˆã¿æœ¬æ–‡ãƒ»å‚è€ƒæ–‡çŒ®ã‚’å†æ§‹æˆ
    final_markdown = ""
    if title:
        final_markdown += title + "\n\n"
    final_markdown += translated_body + "\n\n"
    if reference:
        final_markdown += reference + "\n\n"
    final_markdown += translated_remaining_body

    # ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å…ƒã®ä½ç½®ã«æˆ»ã™
    final_markdown = restore_images(final_markdown, images, placeholders)

    print("\nğŸ‰ ç¿»è¨³å®Œäº†ï¼")
    return final_markdown

if __name__ == "__main__":


    load_dotenv()  # .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€

    api_key_mistralai = os.getenv("MISTRALAI_API_KEY")
    api_key_openai = os.getenv("OPENAI_API_KEY")

    # ãƒ­ãƒ¼ã‚«ãƒ«PDFã®ãƒ‘ã‚¹
    file_name = r"AttentionIsAllYouNeed.pdf"
    data_dir = f'data'
    
    ocr_response = ocr_pdf(api_key_mistralai, file_name, data_dir)

    paper_raw_md = get_combined_markdown(ocr_response)

    save_markdown(paper_raw_md, filename="output/test.md")
    
    paper_translated_md = translate_gpt(ocr_response, api_key_openai)

    save_markdown(paper_translated_md, filename="output/test_translated.md")


    